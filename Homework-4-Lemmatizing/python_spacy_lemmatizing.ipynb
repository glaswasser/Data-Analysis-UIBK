{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing strings using spacy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file import File\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "\n",
    "files = os.listdir(\"./texts\")\n",
    "\n",
    "file_paths = [wd+\"/texts/\"+file for file in files]\n",
    "\n",
    "\n",
    "\n",
    "dataframes = [open(file, \"r\").read() for file in file_paths]\n",
    "\n",
    "# short for:\n",
    "#dataframes = []\n",
    "\n",
    "#for file in file_paths:\n",
    "#    with open(file, 'r') as text:\n",
    "#        dataframes.append(text.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up spacy, Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.208950996398926\n",
      "                                             content  \\\n",
      "0  The Project Gutenberg EBook of The Picture of ...   \n",
      "1  ﻿\\nProject Gutenberg's The Hound of the Basker...   \n",
      "2  ﻿\\nThe Project Gutenberg EBook of Adventures o...   \n",
      "\n",
      "                                         file  \\\n",
      "0      wilde_-_the_picture_of_dorian_gray.txt   \n",
      "1   doyle_-_the_hound_of_the_baskervilles.txt   \n",
      "2  twain_-_adventures_of_huckleberry_finn.txt   \n",
      "\n",
      "                                   tokenized_content  \n",
      "0  (The, Project, Gutenberg, EBook, of, The, Pict...  \n",
      "1  (﻿, \\n, Project, Gutenberg, 's, The, Hound, of...  \n",
      "2  (﻿, \\n, The, Project, Gutenberg, EBook, of, Ad...  \n",
      "50.65294599533081\n"
     ]
    }
   ],
   "source": [
    "# run nlp function on dataframe elements\n",
    "df = pd.DataFrame({\"content\": dataframes, \"file\": files})\n",
    "\n",
    "start = time.time()\n",
    "df['tokenized_content'] = [nlp(text) for text in df.content]\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "print(df)\n",
    "\n",
    "\n",
    "# or\n",
    "start = time.time()\n",
    "df['tokenized_content'] = df.content.apply(nlp)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also works using nltk (looks even better and much faster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexanderheinz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7803802490234375\n",
      "                                             content  \\\n",
      "0  The Project Gutenberg EBook of The Picture of ...   \n",
      "1  ﻿\\nProject Gutenberg's The Hound of the Basker...   \n",
      "2  ﻿\\nThe Project Gutenberg EBook of Adventures o...   \n",
      "\n",
      "                                         file  \\\n",
      "0      wilde_-_the_picture_of_dorian_gray.txt   \n",
      "1   doyle_-_the_hound_of_the_baskervilles.txt   \n",
      "2  twain_-_adventures_of_huckleberry_finn.txt   \n",
      "\n",
      "                                   tokenized_content  \n",
      "0  [The, Project, Gutenberg, EBook, of, The, Pict...  \n",
      "1  [﻿, Project, Gutenberg, 's, The, Hound, of, th...  \n",
      "2  [﻿, The, Project, Gutenberg, EBook, of, Advent...  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "df = pd.DataFrame({\"content\": dataframes, \"file\": files})\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df['tokenized_content'] = [nltk.word_tokenize(text) for text in df.content]\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  The Project Gutenberg EBook of The Picture of ...   \n",
      "1  ﻿\\nProject Gutenberg's The Hound of the Basker...   \n",
      "2  ﻿\\nThe Project Gutenberg EBook of Adventures o...   \n",
      "\n",
      "                                         file  \\\n",
      "0      wilde_-_the_picture_of_dorian_gray.txt   \n",
      "1   doyle_-_the_hound_of_the_baskervilles.txt   \n",
      "2  twain_-_adventures_of_huckleberry_finn.txt   \n",
      "\n",
      "                                   tokenized_content  \\\n",
      "0  [The, Project, Gutenberg, EBook, of, The, Pict...   \n",
      "1  [﻿, Project, Gutenberg, 's, The, Hound, of, th...   \n",
      "2  [﻿, The, Project, Gutenberg, EBook, of, Advent...   \n",
      "\n",
      "                                  lemmatized_content  \\\n",
      "0  [the, Project, Gutenberg, ebook, of, the, Pict...   \n",
      "1  [﻿, Project, Gutenberg, 's, the, Hound, of, th...   \n",
      "2  [﻿, the, Project, Gutenberg, ebook, of, Advent...   \n",
      "\n",
      "                                   unique_lemmatized  unique_lemma_count  \n",
      "0  {asphodel, sharp, an, sickly, don't, Egad, zel...                8069  \n",
      "1  {sharp, an, shores, Give, self-respect, equipp...                7025  \n",
      "2  {State, sharp, an, sickly, don't, shores, intr...                8771  \n"
     ]
    }
   ],
   "source": [
    "df[\"lemmatized_content\"] = df['content'].apply(lambda x: \" \".join([y.lemma_ for y in nlp(x)])).str.split()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the lemmas split like the tokens as new column. \n",
    "\n",
    "Next we count the unique lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  The Project Gutenberg EBook of The Picture of ...   \n",
      "1  ﻿\\nProject Gutenberg's The Hound of the Basker...   \n",
      "2  ﻿\\nThe Project Gutenberg EBook of Adventures o...   \n",
      "\n",
      "                                         file  \\\n",
      "0      wilde_-_the_picture_of_dorian_gray.txt   \n",
      "1   doyle_-_the_hound_of_the_baskervilles.txt   \n",
      "2  twain_-_adventures_of_huckleberry_finn.txt   \n",
      "\n",
      "                                   tokenized_content  \\\n",
      "0  [The, Project, Gutenberg, EBook, of, The, Pict...   \n",
      "1  [﻿, Project, Gutenberg, 's, The, Hound, of, th...   \n",
      "2  [﻿, The, Project, Gutenberg, EBook, of, Advent...   \n",
      "\n",
      "                                  lemmatized_content  \\\n",
      "0  [the, Project, Gutenberg, ebook, of, the, Pict...   \n",
      "1  [﻿, Project, Gutenberg, 's, the, Hound, of, th...   \n",
      "2  [﻿, the, Project, Gutenberg, ebook, of, Advent...   \n",
      "\n",
      "                                   unique_lemmatized  unique_lemma_count  \n",
      "0  {asphodel, sharp, an, sickly, don't, doesn't, ...                5864  \n",
      "1  {Marcini, impression, breed, waist, starve, wi...                4843  \n",
      "2  {you”--tosse, State, sharp, an, sickly, don't,...                5813  \n"
     ]
    }
   ],
   "source": [
    "# keep unique lemmas\n",
    "df[\"unique_lemmatized\"] = [set(content) for content in df.lemmatized_content]\n",
    "# count unique lemmas\n",
    "df[\"unique_lemma_count\"] = [len(elements) for elements in df.unique_lemmatized]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Oscar wilde has had a competition against Mark Twain, but won this competition with 5864 unique lemmas!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
